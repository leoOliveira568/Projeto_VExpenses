{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4263faef-7c87-4069-b56f-8a7eddf04a75",
   "metadata": {},
   "source": [
    "# Célula 1: Importando bibliotecas e carregando os dados\n",
    "\n",
    "Nesta célula, você vai importar as bibliotecas necessárias para manipulação de dados, construção de modelos de Machine Learning e avaliação dos resultados. Usará o `pandas` para trabalhar com os dados, o `numpy` para funções matemáticas, o `matplotlib` para visualização e o `scikit-learn` para os modelos e métricas. Você também vai carregar o dataset e mostrar as primeiras linhas para garantir que ele foi lido corretamente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36151ee4-4b19-41c8-82fb-b2d2b239ec99",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Carregando o dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Exibindo as primeiras linhas do dataset\u001b[39;00m\n\u001b[0;32m     16\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregando o dataset\n",
    "dataset = pd.read_csv('Dataset.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas do dataset\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd5962-30dd-4ff7-9e1f-2716840dc14d",
   "metadata": {},
   "source": [
    "# Célula 2: Pré-processamento dos dados\n",
    "\n",
    "Nessa célula, o pré-processamento dos dados foi realizado. Os valores ausentes nas colunas numéricas foram preenchidos com a média dos valores. A coluna categórica 'Gênero' teve os valores ausentes substituídos pela moda (valor mais frequente). Na coluna 'Tempo no Site', valores negativos foram substituídos por `NaN` e, posteriormente, preenchidos pela média, a fim de evitar distorções nos resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79014233-8a58-4d50-b6ac-4532a2d06398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os dados e substituindo valores ausentes\n",
    "dataset['Idade'] = dataset['Idade'].fillna(dataset['Idade'].mean())\n",
    "dataset['Renda Anual (em $)'] = dataset['Renda Anual (em $)'].fillna(dataset['Renda Anual (em $)'].mean())\n",
    "dataset['Gênero'] = dataset['Gênero'].fillna(dataset['Gênero'].mode()[0])\n",
    "dataset['Anúncio Clicado'] = dataset['Anúncio Clicado'].fillna(dataset['Anúncio Clicado'].mode()[0])\n",
    "\n",
    "# Para \"Tempo no Site (min)\", se -1 for um valor que indica erro, substituímos por NaN e depois pelo valor médio\n",
    "dataset['Tempo no Site (min)'] = dataset['Tempo no Site (min)'].replace(-1, np.nan)\n",
    "dataset['Tempo no Site (min)'] = dataset['Tempo no Site (min)'].fillna(dataset['Tempo no Site (min)'].mean())\n",
    "\n",
    "# Exibindo as primeiras linhas após o tratamento de valores ausentes\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6f44d-a627-462c-b629-44223386cc4a",
   "metadata": {},
   "source": [
    "# Célula 3: Conversão de variáveis categóricas\n",
    "\n",
    "Nesta célula, as variáveis categóricas foram convertidas para valores numéricos. A coluna 'Gênero' foi transformada usando o `LabelEncoder`, enquanto a coluna 'Anúncio Clicado' foi binarizada, com 'Sim' transformado em 1 e 'Não' em 0. Esse procedimento é necessário para que os modelos de Machine Learning consigam processar essas variáveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33e1f1-3fa6-45d9-bc89-b624ef904a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo variáveis categóricas para valores numéricos\n",
    "le = LabelEncoder()\n",
    "dataset['Gênero'] = le.fit_transform(dataset['Gênero'])\n",
    "dataset['Anúncio Clicado'] = dataset['Anúncio Clicado'].apply(lambda x: 1 if x == 'Sim' else 0)\n",
    "\n",
    "# Exibindo as primeiras linhas após a conversão\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740145c-2b99-40b9-a5dc-18bcfdbd26ec",
   "metadata": {},
   "source": [
    "# Célula 4: Preparação dos dados para o modelo\n",
    "\n",
    "Aqui, as variáveis independentes (X) foram separadas da variável alvo (y). Além disso, os dados foram normalizados usando o `StandardScaler`, o que foi feito para garantir que todas as variáveis estivessem na mesma escala. Isso ajuda a melhorar a performance de certos modelos, como a Regressão Logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad88e5d-11ab-4b14-a026-ca073e639706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as variáveis independentes (X) e a variável alvo (y)\n",
    "X = dataset.drop(columns=['Compra (0 ou 1)'])\n",
    "y = dataset['Compra (0 ou 1)']\n",
    "\n",
    "# Normalizando os dados para melhorar a performance da Regressão Logística\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d5d7e-fb12-4c81-a52c-87a3530d75d8",
   "metadata": {},
   "source": [
    "# Célula 5: Balanceamento de classes com SMOTE\n",
    "\n",
    "Nesta célula, foi aplicado o SMOTE (Synthetic Minority Over-sampling Technique) para balancear as classes do dataset. O SMOTE gerou exemplos sintéticos da classe minoritária, equilibrando assim as classes e evitando que o modelo seja enviesado para a classe majoritária.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d6448-a877-4b61-af47-3ffd1d33395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceando as classes com SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926807cd-24a4-44df-bb27-e3f75c2b945e",
   "metadata": {},
   "source": [
    "# Célula 6: Divisão dos dados em treino e teste\n",
    "\n",
    "Nessa célula, o dataset foi dividido em duas partes: 80% para treino e 20% para teste. Isso permite que o modelo seja treinado com uma parte dos dados e avaliado com outra parte, que não foi utilizada durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b3b16-2e14-446a-82f2-de7d4b696f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8913955-9925-487b-95d7-4304b6a95ae8",
   "metadata": {},
   "source": [
    "# Célula 7: Definição dos modelos e ajuste de hiperparâmetros\n",
    "\n",
    "Nessa célula, os modelos que serão utilizados foram definidos: Random Forest, Gradient Boosting, Árvore de Decisão e Regressão Logística. Para cada modelo, o `GridSearchCV` foi utilizado para buscar os melhores hiperparâmetros, usando validação cruzada. Esse procedimento ajuda a otimizar a performance dos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffad12-bcef-4646-a162-eab05e3b0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando os modelos\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Árvore de Decisão\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Regressão Logística\": LogisticRegression(max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Definindo a validação cruzada\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Ajuste de Hiperparâmetros com GridSearchCV para Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros para Random Forest:\")\n",
    "print(grid_rf.best_params_)\n",
    "\n",
    "# Ajuste de Hiperparâmetros com GridSearchCV para Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros para Gradient Boosting:\")\n",
    "print(grid_gb.best_params_)\n",
    "\n",
    "# Ajuste de Hiperparâmetros com GridSearchCV para Árvore de Decisão\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros para Árvore de Decisão:\")\n",
    "print(grid_dt.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c8aa7-8c67-472d-9fa1-849de059dd15",
   "metadata": {},
   "source": [
    "# Célula 8: Avaliação dos modelos e exibição de resultados\n",
    "\n",
    "Aqui, os modelos foram treinados e avaliados com base nas previsões feitas para os dados de teste. A matriz de confusão e o relatório de classificação foram gerados para cada modelo. Também foi realizada uma validação cruzada para avaliar o desempenho dos modelos e as métricas de desempenho, como acurácia, precisão, recall e F1-score, foram calculadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ff40b-e4d6-4559-9de5-df716cdeed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para exibir importância ou coeficientes das variáveis\n",
    "def mostrar_importancia_variaveis(modelo, X, nome_modelo):\n",
    "    try:\n",
    "        importancias = modelo.feature_importances_  # Para modelos baseados em árvores\n",
    "        importancia_df = pd.DataFrame({\n",
    "            \"Variável\": X.columns,\n",
    "            \"Importância\": importancias\n",
    "        }).sort_values(by=\"Importância\", ascending=False)\n",
    "        \n",
    "        print(f\"Importância das Variáveis ({nome_modelo}):\")\n",
    "        print(importancia_df)\n",
    "        print(\"\\n\")\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            coeficientes = modelo.coef_[0]  # Para Regressão Logística\n",
    "            coef_df = pd.DataFrame({\n",
    "                \"Variável\": X.columns,\n",
    "                \"Coeficiente\": coeficientes\n",
    "            }).sort_values(by=\"Coeficiente\", ascending=False)\n",
    "            \n",
    "            print(f\"Coeficientes das Variáveis ({nome_modelo}):\")\n",
    "            print(coef_df)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"O modelo {nome_modelo} não suporta cálculo de importância de variáveis diretamente.\")\n",
    "            print(e)\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Armazenando as métricas dos modelos para comparação\n",
    "results = {}\n",
    "\n",
    "# Avaliação de todos os modelos\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"Avaliação do modelo {name} com ajuste de hiperparâmetros:\")\n",
    "    \n",
    "    # Se o modelo tem GridSearchCV aplicado, use o melhor modelo\n",
    "    if name == \"Random Forest\":\n",
    "        classifier = grid_rf.best_estimator_\n",
    "    elif name == \"Gradient Boosting\":\n",
    "        classifier = grid_gb.best_estimator_\n",
    "    elif name == \"Árvore de Decisão\":\n",
    "        classifier = grid_dt.best_estimator_\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Exibindo as métricas de desempenho\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Acurácia do {name}: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Validação cruzada\n",
    "    cv_scores = cross_val_score(classifier, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Validação cruzada (Acurácia média) para {name}: {np.mean(cv_scores):.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Exibindo importância das variáveis\n",
    "    print(f\"Analisando a importância das variáveis para o modelo {name}:\")\n",
    "    mostrar_importancia_variaveis(classifier, pd.DataFrame(X_res, columns=X.columns), name)\n",
    "\n",
    "    # Armazenando as métricas de Acurácia, Precisão, Recall e F1-Score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\n",
    "        \"Acurácia\": acc,\n",
    "        \"Precisão\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# Convertendo os resultados para DataFrame para visualização\n",
    "resultados_df = pd.DataFrame(results).T\n",
    "\n",
    "# Exibindo a tabela comparativa\n",
    "print(\"Comparação de Modelos:\")\n",
    "print(resultados_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3dd08-bb46-46f3-bf8d-f30f72c0ba70",
   "metadata": {},
   "source": [
    "# Célula 9: Plotagem dos resultados\n",
    "\n",
    "Por fim, um gráfico de barras foi gerado para comparar as métricas de Acurácia, Precisão, Recall e F1-Score dos modelos. Esse gráfico facilita a visualização do desempenho dos modelos em relação às métricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d847bf-6dfa-4462-a56c-c1e2390d3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando gráfico de barras para comparar as métricas\n",
    "resultados_df.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Comparação de Modelos - Acurácia, Precisão, Recall e F1-Score\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.xlabel(\"Modelos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
